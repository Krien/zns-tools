diff --git a/db/compaction/compaction.h b/db/compaction/compaction.h
index ee8639601..f8d926701 100644
--- a/db/compaction/compaction.h
+++ b/db/compaction/compaction.h
@@ -408,6 +408,9 @@ class Compaction {
                                       const int start_level,
                                       const int output_level);
 
+    VersionStorageInfo* StorageInfo() { return input_vstorage_; }
+
+
  private:
   // mark (or clear) all files that are being compacted
   void MarkFilesBeingCompacted(bool mark_as_compacted);
diff --git a/db/compaction/compaction_job.cc b/db/compaction/compaction_job.cc
index 331be915e..6041f46ea 100644
--- a/db/compaction/compaction_job.cc
+++ b/db/compaction/compaction_job.cc
@@ -8,7 +8,7 @@
 // found in the LICENSE file. See the AUTHORS file for names of contributors.
 
 #include "db/compaction/compaction_job.h"
-
+#include <sys/wait.h>
 #include <algorithm>
 #include <cinttypes>
 #include <memory>
@@ -620,6 +620,32 @@ Status CompactionJob::Run() {
   log_buffer_->FlushBufferToLog();
   LogCompaction();
 
+  {
+  	char *args[2];
+  	args[0] = const_cast<char*>("/tmp/rocksdb/scripts/report_rocksdb_status.sh");
+  	args[1] = new char[64];
+  	memset(args[1], 0, 64);
+  	snprintf(args[1], 63, "before_compaction_%u", job_id_);
+  	pid_t pid = fork();
+  	if (pid == -1)
+  	{
+		// error, failed to fork()
+  	} 
+  	else if (pid > 0)
+  	{
+		int stat;
+		waitpid(pid, &stat, 0);
+  	}
+  	else 
+ 	{
+  		execve(args[0], args, NULL);
+    		// we are the child
+    		_exit(EXIT_FAILURE);   // exec never ret
+ 	 }
+  	fprintf(stderr, "Analysed files before: %s\n", args[0]);
+  	delete[] args[1];
+  }
+
   const size_t num_threads = compact_->sub_compact_states.size();
   assert(num_threads > 0);
   const uint64_t start_micros = db_options_.clock->NowMicros();
@@ -783,6 +809,35 @@ Status CompactionJob::Run() {
     }
   }
 
+  {
+  	char *args[2];
+  	args[0] = const_cast<char*>("/tmp/rocksdb/scripts/report_rocksdb_status.sh");
+  	args[1] = new char[64];
+  	memset(args[1], 0, 64);
+  	snprintf(args[1], 63, "after_compaction_%u", job_id_);
+  	pid_t pid = fork();
+  	if (pid == -1)
+  	{
+		// error, failed to fork()
+		fprintf(stderr, "error forking!\n");
+  	} 
+  	else if (pid > 0)
+  	{
+		int stat;
+		waitpid(pid, &stat, 0);
+  	}
+  	else 
+ 	{
+  		execve(args[0], args, NULL);
+    		// we are the child
+    		_exit(EXIT_FAILURE);   // exec never ret
+ 	 }
+  	fprintf(stderr, "Analysed files after: %s\n", args[1]);
+  	delete[] args[1];
+  }
+
+
+
   ReleaseSubcompactionResources();
   TEST_SYNC_POINT("CompactionJob::ReleaseSubcompactionResources:0");
   TEST_SYNC_POINT("CompactionJob::ReleaseSubcompactionResources:1");
@@ -805,7 +860,6 @@ Status CompactionJob::Run() {
   RecordCompactionIOStats();
   LogFlush(db_options_.info_log);
   TEST_SYNC_POINT("CompactionJob::Run():End");
-
   compact_->status = status;
   return status;
 }
@@ -2003,7 +2057,44 @@ void CompactionJob::LogCompaction() {
         stream << f->fd.GetNumber();
       }
       stream.EndArray();
+      stream << ("files_L_begin" + std::to_string(compaction->level(i)));
+      stream.StartArray();
+      for (auto f : *compaction->inputs(i)) {
+        stream << f->smallest.DebugString(true);
+      }
+      stream.EndArray();
+      stream << ("files_L_end" + std::to_string(compaction->level(i)));
+      stream.StartArray();
+      for (auto f : *compaction->inputs(i)) {
+        stream << f->largest.DebugString(true);
+      }
+      stream.EndArray();
     }
+      stream << "scores_level";
+      stream.StartArray();
+      for (int i=0; i <compaction->StorageInfo()->MaxOutputLevel(false); i++) {
+       stream << compaction->StorageInfo()->CompactionScoreLevelCopy(i);
+      }
+      stream.EndArray();
+
+       stream << "scores_";
+      stream.StartArray();
+      for (int i=0; i <compaction->StorageInfo()->MaxOutputLevel(false); i++) {
+       stream << compaction->StorageInfo()->CompactionScoreCopy(i);
+      }
+      stream.EndArray();
+
+      stream << "files";
+      stream.StartArray();
+      for (int i=0; i <compaction->StorageInfo()->MaxOutputLevel(false); i++) {
+ const std::vector<FileMetaData*> filee = compaction->StorageInfo()->LevelFiles(i);
+	 for (const auto& f : filee) {
+	    stream << ("L" + std::to_string(i) + "-" +  std::to_string(f->compensated_file_size) + "-" + std::to_string(f->fd.GetNumber()) + "-" + (f->being_compacted ? "1" : "0"));
+	 }
+      }
+      stream.EndArray();
+
+
     stream << "score" << compaction->score() << "input_data_size"
            << compaction->CalculateTotalInputSize() << "oldest_snapshot_seqno"
            << (existing_snapshots_.empty()
diff --git a/db/compaction/compaction_picker_level.cc b/db/compaction/compaction_picker_level.cc
index 2162d30a3..45433f4da 100644
--- a/db/compaction/compaction_picker_level.cc
+++ b/db/compaction/compaction_picker_level.cc
@@ -501,6 +501,7 @@ Compaction* LevelCompactionBuilder::PickCompaction() {
 }
 
 Compaction* LevelCompactionBuilder::GetCompaction() {
+  vstorage_->PinCompactHack(vstorage_->MaxOutputLevel(false));
   auto c = new Compaction(
       vstorage_, ioptions_, mutable_cf_options_, mutable_db_options_,
       std::move(compaction_inputs_), output_level_,
diff --git a/db/db_impl/db_impl_compaction_flush.cc b/db/db_impl/db_impl_compaction_flush.cc
index da43d609d..bcfeb4992 100644
--- a/db/db_impl/db_impl_compaction_flush.cc
+++ b/db/db_impl/db_impl_compaction_flush.cc
@@ -3457,11 +3457,24 @@ Status DBImpl::BackgroundCompaction(bool* made_progress,
     c->column_family_data()->internal_stats()->IncBytesMoved(c->output_level(),
                                                              moved_bytes);
     {
+	    std::stringstream ss;
+	ss << "[";
+	bool first = true;
+       for (size_t i = 0; i < c->num_input_levels(); ++i) {
+	      for (auto f : *c->inputs(i)) {
+		if (!first) {
+		  ss << ",";
+		}
+		first = false;
+        	ss << f->fd.GetNumber();
+              }
+	  }
+      ss << "]";
       event_logger_.LogToBuffer(log_buffer)
           << "job" << job_context->job_id << "event"
           << "trivial_move"
-          << "destination_level" << c->output_level() << "files" << moved_files
-          << "total_files_size" << moved_bytes;
+          << "destination_level" << c->output_level() << "file_nums" << moved_files
+          << "total_files_size" << moved_bytes << "files" << ss.str();
     }
     ROCKS_LOG_BUFFER(
         log_buffer,
diff --git a/db/flush_job.cc b/db/flush_job.cc
index 8193f594f..8101ae2fc 100644
--- a/db/flush_job.cc
+++ b/db/flush_job.cc
@@ -13,6 +13,8 @@
 #include <cinttypes>
 #include <vector>
 
+#include <sys/wait.h>
+
 #include "db/builder.h"
 #include "db/db_iter.h"
 #include "db/dbformat.h"
@@ -279,6 +281,31 @@ Status FlushJob::Run(LogsWithPrepTracker* prep_tracker, FileMetaData* file_meta,
     base_->Unref();
     s = Status::OK();
   } else {
+   {
+	   char *args[2];
+	   args[0] = const_cast<char*>("/tmp/rocksdb/scripts/report_rocksdb_status.sh");
+	   args[1] = new char[64];
+	   memset(args[1], 0, 64);
+	   snprintf(args[1], 63, "before_flush_%u", job_context_->job_id);
+	   pid_t pid = fork();
+	   if (pid == -1)
+	   {
+		       // error, failed to fork()
+	   }
+	   else if (pid > 0)
+	   {
+	   int stat;
+           waitpid(pid, &stat, 0);
+	   }
+           else
+	   {
+	   execve(args[0], args, NULL);
+	   // we are the child
+	   _exit(EXIT_FAILURE);   // exec never ret
+           }
+           fprintf(stderr, "Analysed flush files before: %s\n", args[0]);
+           delete[] args[1];
+           }
     // This will release and re-acquire the mutex.
     s = WriteLevel0Table();
   }
@@ -310,6 +337,33 @@ Status FlushJob::Run(LogsWithPrepTracker* prep_tracker, FileMetaData* file_meta,
   }
   RecordFlushIOStats();
 
+{
+	   char *args[2];
+	   args[0] = const_cast<char*>("/tmp/rocksdb/scripts/report_rocksdb_status.sh");
+	   args[1] = new char[64];
+	   memset(args[1], 0, 64);
+	   snprintf(args[1], 63, "after_flush_%u", job_context_->job_id);
+	   pid_t pid = fork();
+	   if (pid == -1)
+	   {
+		       // error, failed to fork()
+	   }
+	   else if (pid > 0)
+	   {
+	   int stat;
+           waitpid(pid, &stat, 0);
+	   }
+           else
+	   {
+	   execve(args[0], args, NULL);
+	   // we are the child
+	   _exit(EXIT_FAILURE);   // exec never ret
+           }
+           fprintf(stderr, "Analysed flush files after: %s\n", args[0]);
+           delete[] args[1];
+           }
+
+
   // When measure_io_stats_ is true, the default 512 bytes is not enough.
   auto stream = event_logger_->LogToBuffer(log_buffer_, 1024);
   stream << "job" << job_context_->job_id << "event"
diff --git a/db/version_set.h b/db/version_set.h
index 9c428526c..0757600bb 100644
--- a/db/version_set.h
+++ b/db/version_set.h
@@ -249,9 +249,20 @@ class VersionStorageInfo {
 
   // Return level number that has idx'th highest score
   int CompactionScoreLevel(int idx) const { return compaction_level_[idx]; }
+  int CompactionScoreLevelCopy(int idx) const { return compaction_level_copy_[idx]; }
 
   // Return idx'th highest score
   double CompactionScore(int idx) const { return compaction_score_[idx]; }
+  double CompactionScoreCopy(int idx) const { return compaction_score_copy_[idx]; }
+
+  void PinCompactHack(int idx) {
+	compaction_level_copy_.clear();
+	compaction_score_copy_.clear();
+	for (int i=0; i < idx; i++) {
+		compaction_level_copy_.push_back(compaction_level_[i]);
+		compaction_score_copy_.push_back(compaction_score_[i]);
+	}
+  }
 
   void GetOverlappingInputs(
       int level, const InternalKey* begin,  // nullptr means before all keys
@@ -712,7 +723,9 @@ class VersionStorageInfo {
   // The most critical level to be compacted is listed first
   // These are used to pick the best compaction level
   std::vector<double> compaction_score_;
+  std::vector<double> compaction_score_copy_;
   std::vector<int> compaction_level_;
+  std::vector<int> compaction_level_copy_;
   int l0_delay_trigger_count_ = 0;  // Count used to trigger slow down and stop
                                     // for number of L0 files.
 
