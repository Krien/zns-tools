{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4485690",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331a3c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import json\n",
    "import pprint\n",
    "\n",
    "import sys, getopt\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0118258",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20813b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs\n",
    "filesystem_type = 'f2fs'\n",
    "data_dir = '/tmp/rocksdb/f2fs/'\n",
    "\n",
    "# ZNS (true for the used device/namespace)\n",
    "cap = 131072\n",
    "page_size = 512\n",
    "level0 = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116fe380",
   "metadata": {},
   "source": [
    "# Gather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4a03fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path_dict = {\n",
    "    'f2fs': '/mnt/f2fs/db0/'\n",
    "} \n",
    "db_path = db_path_dict[filesystem_type] if filesystem_type in db_path_dict else '/rocksdbtest/dbbench/'\n",
    "\n",
    "compactions = {}\n",
    "jobs = []\n",
    "table_levels = {}\n",
    "level_table = {} \n",
    "flushes = []\n",
    "trivial_compactions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a5ea96",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data = []\n",
    "with open(f'{data_dir}/LOG', 'r') as f: #open the file\n",
    "    log_data = f.readlines()\n",
    "log_data = [d.rstrip() for d in log_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0a27e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first pass: create compactions\n",
    "for log_data_line in log_data:\n",
    "    if ': \"compaction_start' in log_data_line:\n",
    "        compaction = json.loads(log_data_line[log_data_line.find('{'):])\n",
    "        date = log_data_line.split(' ')[0]\n",
    "        datetime_object = datetime.strptime(date, '%Y/%m/%d-%H:%M:%S.%f')\n",
    "        compaction_level = compaction['scores_level'][0]\n",
    "        jobs.append(compaction['job'])\n",
    "        compactions[compaction['job']] = {\n",
    "            'type': 'compaction',\n",
    "            'scores': compaction['scores_']\n",
    "            ,'scores_levels': compaction['scores_level']\n",
    "            ,'high_score': compaction['score']\n",
    "            ,'level': compaction_level\n",
    "            ,'from': compaction[f'files_L{compaction_level}']\n",
    "            ,'into': compaction[f'files_L{compaction_level+1}'] if f'files_L{compaction_level+1}' in compaction else []\n",
    "            ,'file_snapshot': [{\n",
    "                'level': int(c.split('-')[0][1]), \n",
    "                'size':c.split('-')[1], \n",
    "                'id':c.split('-')[2],\n",
    "                'compacting':c.split('-')[3]\n",
    "            }  for c in compaction['files']]\n",
    "            ,\"creation\": []\n",
    "            ,\"deletion\": []\n",
    "            ,\"zenfs_file_snapshot_before\": []\n",
    "            ,\"zenfs_file_snapshot_after\": []\n",
    "            ,\"zones_snapshot_before\": []\n",
    "            ,\"zones_snapshot_after\": []\n",
    "            ,\"datetime\": datetime_object\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e924d05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# second pass: flush pass\n",
    "for log_data_line in log_data:\n",
    "    if 'job\":' in log_data_line:\n",
    "        if \"flush_finished\" in log_data_line:\n",
    "            creation = json.loads(log_data_line[log_data_line.find('{'):])\n",
    "            jobs.append(creation['job'])\n",
    "            date = log_data_line.split(' ')[0]\n",
    "            datetime_object = datetime.strptime(date, '%Y/%m/%d-%H:%M:%S.%f')\n",
    "            compactions[creation['job']] = {\n",
    "                'type': 'flush'\n",
    "                ,'creation': []\n",
    "                ,\"f2fs_file_snapshot_before\": []\n",
    "                ,\"f2fs_file_snapshot_after\": []\n",
    "                ,\"zones_snapshot_before\": []\n",
    "                ,\"zones_snapshot_after\": []\n",
    "                ,\"datetime\": datetime_object\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d59ab06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# third pass: assign table creations/deletions to jobs\n",
    "for log_data_line in log_data:\n",
    "    if 'job\":' in log_data_line:\n",
    "        if \"table_file_creation\" in log_data_line:\n",
    "            # Determine name of file\n",
    "            table_creation = json.loads(log_data_line[log_data_line.find('{'):])\n",
    "            job_id = table_creation['job']\n",
    "            table_file_id = str(table_creation['file_number'])\n",
    "            table_file_preamble = '000000'[:6-len(table_file_id)]\n",
    "            table_file_name = f'{db_path}{table_file_preamble}{table_file_id}.sst'\n",
    "            # Assign to table/flush dictionary\n",
    "            if table_file_name not in table_levels:\n",
    "                table_levels[table_file_name] = []\n",
    "            if job_id not in compactions or compactions[job_id]['type'] != 'compaction':\n",
    "                table_levels[table_file_name].append((job_id, level0))  \n",
    "                if job_id in compactions and compactions[job_id]['type'] == 'flush':\n",
    "                    flushes.append((job_id, table_file_id))\n",
    "                    compactions[job_id]['creation'].append(table_file_id)\n",
    "            else:\n",
    "                compactions[job_id]['creation'].append(table_file_id)\n",
    "                table_levels[table_file_name].append((job_id, compactions[job_id]['level']+1))\n",
    "\n",
    "        if \"table_file_deletion\" in log_data_line:\n",
    "            table_file_deletion = json.loads(log_data_line[log_data_line.find('{'):])\n",
    "            table_file_deletion_id = table_file_deletion['file_number']\n",
    "            job_id = table_file_deletion['job']\n",
    "            if job_id not in compactions or compactions[job_id]['type'] != 'compaction':\n",
    "                continue\n",
    "            compactions[job_id]['deletion'].append(table_file_deletion_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c2acd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fourth pass: trivial moves to higher levels\n",
    "for log_data_line in log_data:\n",
    "    if 'job\":' in log_data_line:\n",
    "        if \"trivial_move\" in log_data_line:\n",
    "            # Load\n",
    "            trivial_move = json.loads(log_data_line[log_data_line.find('{'):])\n",
    "            job_id = trivial_move['job']\n",
    "            next_level = trivial_move['destination_level']\n",
    "            date = log_data_line.split(' ')[0]\n",
    "            datetime_object = datetime.strptime(date, '%Y/%m/%d-%H:%M:%S.%f')\n",
    "            compactions[job_id] = {\n",
    "                'type': 'trivial_move'\n",
    "                ,'creation': []\n",
    "                ,'level': next_level-1\n",
    "                ,\"datetime\": datetime_object\n",
    "            }\n",
    "            # Monkey path the json so that we can read the files\n",
    "            tmp_moved_file_ids = trivial_move['files']\n",
    "            moved_file_ids = json.loads(\"{\" + f'\"hack\": {tmp_moved_file_ids}' + \"}\")['hack']\n",
    "            compactions[job_id]['creation'] = moved_file_ids\n",
    "            # Create names for all files\n",
    "            for moved_file_id in moved_file_ids:\n",
    "                table_file_id = str(moved_file_id)                \n",
    "                table_file_preamble = '000000'[:6-len(table_file_id)]\n",
    "                table_file_name = f'{db_path}{table_file_preamble}{table_file_id}.sst'\n",
    "                if table_file_name not in table_levels:\n",
    "                    table_levels[table_file_name] = []\n",
    "                table_levels[table_file_name].append((job_id, next_level))  \n",
    "                trivial_compactions.append((job_id, moved_file_id, next_level))\n",
    "            # Assign to jobs\n",
    "            jobs.append(job_id)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7caa63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fifth pass: zone resets\n",
    "zone_resets = []\n",
    "with open(f'{data_dir}/bpftrace_reset', 'r') as f: #open the file\n",
    "    reset_data = f.readlines()\n",
    "    reset_data = [d.rstrip() for d in reset_data]\n",
    "    line = -1\n",
    "    for reset_data_line in reset_data:\n",
    "        line = line + 1\n",
    "        if line == 0:\n",
    "            continue\n",
    "        date = reset_data_line.split(' ')[0]\n",
    "        datetime_object = datetime.strptime(date, '%Y/%m/%d-%H:%M:%S:%f') \n",
    "        zone = int(int(reset_data_line.split(' ')[2]) / cap)\n",
    "        zone_resets.append( (datetime_object, zone) )\n",
    "\n",
    "sorted_jobs = sorted(jobs)\n",
    "\n",
    "ptr_jobs   = 0\n",
    "ptr_resets = 0\n",
    "\n",
    "while ptr_jobs < len(sorted_jobs):\n",
    "    compactions[sorted_jobs[ptr_jobs]]['past_resets'] = []\n",
    "    while ptr_resets < len(zone_resets) and zone_resets[ptr_resets][0] < compactions[sorted_jobs[ptr_jobs]]['datetime']:\n",
    "        compactions[sorted_jobs[ptr_jobs]]['past_resets'].append( (f'{zone_resets[ptr_resets][0]}', zone_resets[ptr_resets][1]) )\n",
    "        ptr_resets = ptr_resets + 1\n",
    "    new_date = compactions[sorted_jobs[ptr_jobs]]['datetime']\n",
    "    compactions[sorted_jobs[ptr_jobs]]['datetime'] = f'{new_date}'\n",
    "    ptr_jobs = ptr_jobs + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662d6af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZenFS snapshot file parsing\n",
    "if filesystem_type == 'zenfs':\n",
    "    # before \n",
    "    for job in jobs:\n",
    "        # We only have snapshot support for compactions\n",
    "        if compactions[job]['type'] != 'compaction':\n",
    "            continue\n",
    "        \n",
    "        # Parse before and after compactions\n",
    "        with open(f'{data_dir}/files_before_compaction_{job}', 'r') as f: #open the file\n",
    "            zenfs_file_data = f.read()\n",
    "            # Init files\n",
    "            compactions[job]['zenfs_file_snapshot_before'] =  [\n",
    "                {'filename': f['filenames'][0]['filename'], \n",
    "                 'hint':f['hint'], \n",
    "                 'size': f['size'],\n",
    "                 'zones': {(int(e['start']/(cap*512))):0 for e in f['extents']}, \n",
    "                } for f in json.loads(zenfs_file_data)['files']\n",
    "            ]\n",
    "            # Patch zone occupancy\n",
    "            file_counter = 0\n",
    "            for f in json.loads(f.read())['files']:\n",
    "                print(f)\n",
    "                for e in f['extents']:\n",
    "                    zone = (int(e['start']/(cap*512)))\n",
    "                    compactions[job]['zenfs_file_snapshot_before'][file_counter]['zones'][zone] = compactions[job]['zenfs_file_snapshot_before'][file_counter]['zones'][zone]  + e['length']\n",
    "                file_counter = file_counter+1\n",
    "        with open(f'{data_dir}/files_after_compaction_{job}', 'r') as f: #open the file\n",
    "            zenfs_file_data = f.read()\n",
    "            # Init files\n",
    "            compactions[job]['zenfs_file_snapshot_after'] =  [\n",
    "                {'filename': f['filenames'][0]['filename'], \n",
    "                 'hint':f['hint'], \n",
    "                 'size': f['size'],\n",
    "                 'zones': {(int(e['start']/(cap*512))):0 for e in f['extents']}, \n",
    "                } for f in json.loads(zenfs_file_data)['files']\n",
    "            ]\n",
    "            # Patch zone occupancy\n",
    "            file_counter = 0\n",
    "            for f in json.loads(f.read())['files']:\n",
    "                for e in f['extents']:\n",
    "                    zone = (int(e['start']/(cap*512)))\n",
    "                    compactions[job]['zenfs_file_snapshot_after'][file_counter]['zones'][zone] = compactions[job]['zenfs_file_snapshot_after'][file_counter]['zones'][zone]  + e['length']\n",
    "                file_counter = file_counter+1\n",
    "\n",
    "        # Parse zone info\n",
    "        with open(f'{data_dir}/zones_before_compaction_{job}', 'r') as f: #open the file\n",
    "            zone_data = f.read()\n",
    "            compactions[job]['zones_snapshot_before'] = [{\n",
    "                'state': z['state'], \n",
    "                'wp': z['wp'] - z['slba']\n",
    "            } for z in json.loads(zone_data)['zone_list']]\n",
    "        with open(f'{data_dir}/zones_after_compaction_{job}', 'r') as f: #open the file\n",
    "            zone_data = f.read()\n",
    "            compactions[job]['zones_snapshot_after'] =  [{\n",
    "                'state': z['state'],\n",
    "                'wp': z['wp'] - z['slba']\n",
    "            } for z in json.loads(zone_data)['zone_list']]      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3022c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F2FS snapshot file parsing\n",
    "if filesystem_type == 'f2fs':\n",
    "    for job in jobs:\n",
    "        # We only support compaction snapshots for now\n",
    "        if compactions[job]['type'] != 'compaction' and compactions[job]['type'] != 'flush':\n",
    "            continue\n",
    "        job_name = 'compaction_' if compactions[job]['type'] == 'compaction' else 'flush_'\n",
    "        with open(f'{data_dir}/before_{job_name}{job}', 'r') as f: #open the file\n",
    "            f2fs_snapshot = f.read()\n",
    "            f2fs_snapshot_file_mappings = json.loads(f2fs_snapshot)['zone_file_mappings']\n",
    "            f2fs_snapshot_files = {}\n",
    "            hint = {}\n",
    "            # Reverse indexing\n",
    "            for f2fs_snapshot_file_mapping in f2fs_snapshot_file_mappings:\n",
    "                z = f2fs_snapshot_file_mapping['zone']\n",
    "                files = f2fs_snapshot_file_mapping['files']\n",
    "                for file in files:\n",
    "                    # Special file indicated the hint, not an extent\n",
    "                    if 'ZONE_TYPE' in file:\n",
    "                        hint[int(f2fs_snapshot_file_mapping['zone']) - 1] = file['ZONE_TYPE']\n",
    "                    # Normal extent data\n",
    "                    if 'Type' in file and 'EXTENT' == file['Type']:\n",
    "                        # Extent data\n",
    "                        file_name = file['FILE_NAME']\n",
    "                        extent_start = file['PBAS']\n",
    "                        extent_end = file['PBAE']\n",
    "                        while len(extent_start) < len(extent_end):\n",
    "                            extent_start += '0'\n",
    "                        while len(extent_end) < len(extent_start):\n",
    "                             extent_end += '0'\n",
    "#                         print(extent_start, extent_end)\n",
    "                        extent_start = int(extent_start, 16)     \n",
    "                        extent_end = int(extent_end, 16)\n",
    "                        extent_size = int(file['SIZE'], 16) * page_size\n",
    "                        # Initialize file (note that a file can be present in multiple extents)\n",
    "                        if not file_name in f2fs_snapshot_files:\n",
    "                            f2fs_snapshot_files[file_name] = {\n",
    "                                'filename': file_name,\n",
    "                                'hint': [],\n",
    "                                'size': 0,\n",
    "                                'zones': []\n",
    "                            }\n",
    "                        # Path file data to be integers and alligned to zones\n",
    "                        f2fs_snapshot_files[file_name]['size'] =  f2fs_snapshot_files[file_name]['size']  + extent_size \n",
    "                        while extent_start <= extent_end:\n",
    "#                             print(z, int(extent_start / cap), extent_end / cap, extent_end, job)\n",
    "                            f2fs_snapshot_files[file_name]['zones'].append( int(extent_start / cap) ) \n",
    "                            f2fs_snapshot_files[file_name]['hint'].append( ( int(extent_start / cap), 0, extent_size ) ) \n",
    "                            extent_start = extent_start + cap\n",
    "            \n",
    "            for file_name in f2fs_snapshot_files.keys():\n",
    "                for fh in range(len(f2fs_snapshot_files[file_name]['zones'])):\n",
    "                    ptr = f2fs_snapshot_files[file_name]['hint'][fh][0]\n",
    "                    while ptr not in hint:\n",
    "                        ptr = ptr - 1\n",
    "                    f2fs_snapshot_files[file_name]['hint'][fh] = (f2fs_snapshot_files[file_name]['hint'][fh][0], hint[ptr],f2fs_snapshot_files[file_name]['hint'][fh][2])\n",
    "\n",
    "            # Remove zone duplicates\n",
    "            for file_name, file_data in f2fs_snapshot_files.items():\n",
    "                f2fs_snapshot_files[file_name]['zones'] =  list( dict.fromkeys(f2fs_snapshot_files[file_name]['zones']) )\n",
    "            # Remove duplicate layer of indexing\n",
    "            compactions[job]['f2fs_file_snapshot_before'] =  list(f2fs_snapshot_files.values())\n",
    "            \n",
    "        with open(f'{data_dir}/after_{job_name}{job}', 'r') as f: #open the file\n",
    "            f2fs_snapshot = f.read()\n",
    "            if (f2fs_snapshot ==  ''):\n",
    "                print('empty')\n",
    "                break\n",
    "            f2fs_snapshot_file_mappings = json.loads(f2fs_snapshot)['zone_file_mappings']\n",
    "            f2fs_snapshot_files = {}\n",
    "            # Reverse indexing\n",
    "            hint = {}\n",
    "            for f2fs_snapshot_file_mapping in f2fs_snapshot_file_mappings:\n",
    "                z = f2fs_snapshot_file_mapping['zone']\n",
    "                files = f2fs_snapshot_file_mapping['files']\n",
    "                for file in files:\n",
    "                    # Special file indicated the hint, not an extent\n",
    "                    if 'ZONE_TYPE' in file:\n",
    "                        hint[int(f2fs_snapshot_file_mapping['zone']) - 1] = file['ZONE_TYPE']\n",
    "                        print(int(f2fs_snapshot_file_mapping['zone']) - 1, file['ZONE_TYPE'])\n",
    "                    # Normal extent data\n",
    "                    if 'Type' in file and 'EXTENT' == file['Type']:\n",
    "                        # Extent data\n",
    "                        file_name = file['FILE_NAME']\n",
    "                        extent_start = file['PBAS']\n",
    "                        extent_end = file['PBAE']\n",
    "                        while len(extent_start) < len(extent_end):\n",
    "                            extent_start += '0'\n",
    "                        while len(extent_end) < len(extent_start):\n",
    "                             extent_end += '0'\n",
    "#                         print(extent_start, extent_end)\n",
    "                        extent_start = int(extent_start, 16)     \n",
    "                        extent_end = int(extent_end, 16)\n",
    "                        extent_size = int(file['SIZE'], 16) * page_size\n",
    "                        # Initialize file (note that a file can be present in multiple extents)\n",
    "                        if not file_name in f2fs_snapshot_files:\n",
    "                            f2fs_snapshot_files[file_name] = {\n",
    "                                'filename': file_name,\n",
    "                                'hint': [],\n",
    "                                'size': 0,\n",
    "                                'zones': []\n",
    "                            }\n",
    "                        # Path file data to be integers and alligned to zones\n",
    "                        f2fs_snapshot_files[file_name]['size'] =  f2fs_snapshot_files[file_name]['size']  + extent_size \n",
    "                        while extent_start <= extent_end:\n",
    "#                             print(z, int(extent_start / cap), extent_end / cap, extent_end, job)\n",
    "                            f2fs_snapshot_files[file_name]['zones'].append( int(extent_start / cap) ) \n",
    "                            f2fs_snapshot_files[file_name]['hint'].append( ( int(extent_start / cap), 0,  extent_size) ) \n",
    "                            extent_start = extent_start + cap            # Remove zone duplicates\n",
    "            for file_name in f2fs_snapshot_files.keys():\n",
    "                for fh in range(len(f2fs_snapshot_files[file_name]['zones'])):\n",
    "                    ptr = f2fs_snapshot_files[file_name]['hint'][fh][0]\n",
    "                    while ptr not in hint:\n",
    "                        ptr = ptr - 1\n",
    "                    f2fs_snapshot_files[file_name]['hint'][fh] = (f2fs_snapshot_files[file_name]['hint'][fh][0], hint[ptr], f2fs_snapshot_files[file_name]['hint'][fh][2])\n",
    "\n",
    "            for file_name, file_data in f2fs_snapshot_files.items():\n",
    "                f2fs_snapshot_files[file_name]['zones'] =  list( dict.fromkeys(f2fs_snapshot_files[file_name]['zones']) )\n",
    "            # Remove duplicate layer of indexing\n",
    "            compactions[job]['f2fs_file_snapshot_after'] =  list(f2fs_snapshot_files.values())\n",
    "\n",
    "\n",
    "        with open(f'{data_dir}/zones_before_{job_name}{job}', 'r') as f: #open the file\n",
    "            zone_data = f.read()\n",
    "            compactions[job]['zones_snapshot_before'] = [{\n",
    "                'state': z['state'],\n",
    "                'wp': z['wp'] - z['slba']\n",
    "            } for z in json.loads(zone_data)['zone_list']]\n",
    "        with open(f'{data_dir}/zones_after_{job_name}{job}', 'r') as f: #open the file\n",
    "            zone_data = f.read()\n",
    "            compactions[job]['zones_snapshot_after'] =  [{\n",
    "                'state': z['state'], \n",
    "                'wp': z['wp'] - z['slba']\n",
    "            } for z in json.loads(zone_data)['zone_list']]      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66757695",
   "metadata": {},
   "source": [
    "# Score recreation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a58576",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_bytes_base = 256 * 1048576\n",
    "max_bytes_base = 1024 * 1024\n",
    "\n",
    "for job in jobs:\n",
    "    if 'type' not in compactions[job]:\n",
    "        continue\n",
    "    if compactions[job]['type'] != 'compaction':\n",
    "        continue\n",
    "    their_score = [0] * len(compactions[job]['scores'])\n",
    "    for i in range(len(their_score)):\n",
    "        their_score[compactions[job]['scores_levels'][i]] = compactions[job]['scores'][i]\n",
    "    print(\"their score\", their_score)\n",
    "    # print(compactions[jobs[0]]['zenfs_file_snapshot_before'])\n",
    "    ddd = compactions[job]['f2fs_file_snapshot_before']\n",
    "    score = []\n",
    "    for i in range(5):\n",
    "        score.append(0)\n",
    "    score[0] = [0,0]\n",
    "    for dddd in ddd:\n",
    "        level = -1\n",
    "        if '.sst' not in dddd['filename']:\n",
    "            continue\n",
    "        filename_id = str(int(dddd['filename'].split('/')[-1].split('.sst')[0]))\n",
    "        skip = True\n",
    "        for comp in compactions[job]['file_snapshot']:\n",
    "            if comp['id'] == filename_id:\n",
    "                skip = False\n",
    "        if (skip):\n",
    "            continue\n",
    "        j = len(table_levels[dddd['filename']]) - 1\n",
    "        while j >= 0:\n",
    "            if table_levels[dddd['filename']][j][0] <= job:\n",
    "                level = table_levels[dddd['filename']][j][1]\n",
    "                break\n",
    "            j = j - 1\n",
    "\n",
    "    #     print(dddd['filename'], level)\n",
    "\n",
    "        if level > 0:\n",
    "            score[level] = score[level] + dddd['size']\n",
    "        if level == 0:\n",
    "            score[level][0] = score[level][0] + dddd['size'] \n",
    "            score[level][1] = score[level][1] + 1\n",
    "\n",
    "    score[0] = max(score[0][1] / 4, score[0][0] / max_bytes_base)\n",
    "\n",
    "    for i in range(len(score)-1):\n",
    "        if i == 0:\n",
    "            score[i+1] =  score[i+1] / max_bytes_base\n",
    "    #         print(max_bytes_base)\n",
    "        else:\n",
    "            #print(i, i*4, max_bytes_base, (4 ** (i-1)) * max_bytes_base)\n",
    "    #         print(score[i+1],    (4 ** (i)) * max_bytes_base)            \n",
    "            score[i+1] =  score[i+1] / ((4 ** (i)) * max_bytes_base)\n",
    "    print(\"our score\", score)\n",
    "        \n",
    "    dif = [0] * len(compactions[job]['scores'])\n",
    "    for i in range(len(dif)):\n",
    "        dif[i] = abs(score[i] - their_score[i])\n",
    "    print(\"diff\", dif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9395fe41",
   "metadata": {},
   "source": [
    "# Printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f076dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f43221",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('trivial:')\n",
    "for job in jobs:\n",
    "    if 'trivial_move' not in compactions[job]['type']:\n",
    "        continue\n",
    "    level = compactions[job]['level']\n",
    "    print('    ', job, f'L{level}->L{level+1}:', compactions[job]['creation'], '->', compactions[job]['creation'])\n",
    "    rr = [x[1] for x in compactions[job]['past_resets']]\n",
    "    print('     R:', rr)   \n",
    "\n",
    "print('flush:')\n",
    "for job in jobs:\n",
    "    if 'flush' not in compactions[job]['type']:\n",
    "        continue\n",
    "    print('    ', job, compactions[job]['creation'])\n",
    "    rr = [x[1] for x in compactions[job]['past_resets']]\n",
    "    print('     R:', rr)\n",
    "\n",
    "\n",
    "print('compaction:')\n",
    "for job in jobs:\n",
    "    if 'compaction' not in compactions[job]['type']:\n",
    "        continue\n",
    "    level = compactions[job]['level']\n",
    "    print('    ', job, f'L{level}->L{level+1}:', compactions[job]['from'],'|',  compactions[job]['into'], '->', compactions[job]['creation'])\n",
    "    print('     D:', compactions[job]['deletion'])\n",
    "    rr = [x[1] for x in compactions[job]['past_resets']]\n",
    "    print('     R:', rr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aee51cd",
   "metadata": {},
   "source": [
    "# Generate JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b927407",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../timeline-gen/data.json', 'w') as f:\\n\",\n",
    "    json.dump(compactions, f, indent=4)\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
