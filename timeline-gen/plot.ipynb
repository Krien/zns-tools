{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf6fc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c598a571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set global vars\n",
    "DATA_FILE_JSON = \"data.json\"\n",
    "DATA_FILE_SUMMARY = \"summary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01c8ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the summary to identify files to follow\n",
    "#\n",
    "# Assumes summary format is in this order\n",
    "#     trivial:    -> all trivial moves\n",
    "#                 order of:\n",
    "#                     job [src_level]->[dest_level] [src_table]->[dest_table]\n",
    "#                     R: [zone_resets]\n",
    "#     flush:      -> all flushes\n",
    "#                 order of:\n",
    "#                     job [table]\n",
    "#                     R: [zone_resets]\n",
    "#     compaction: -> all compactions\n",
    "#                 order of:\n",
    "#                     job src_level->dest_level [src_tables] | [merge_tables] -> [dest_tables] \n",
    "#                     D: [table_deletes]\n",
    "#                     R: [zone_resets]\n",
    "\n",
    "summary_dict = dict()\n",
    "\n",
    "with open(DATA_FILE_SUMMARY) as sum_file:\n",
    "    trivial = False\n",
    "    flush = False\n",
    "    compaction = False\n",
    "    summary_dict[\"trivial\"] = dict()\n",
    "    summary_dict[\"flush\"] = dict()\n",
    "    summary_dict[\"compaction\"] = dict()\n",
    "    job_stack = []\n",
    "\n",
    "    \n",
    "    for line in sum_file:\n",
    "        if trivial:\n",
    "            if \"flush:\" in line:\n",
    "                trivial = False\n",
    "                flush = True\n",
    "            else:\n",
    "                line_split = line.split()\n",
    "                if \"R\" in line:\n",
    "                    summary_dict[\"trivial\"][job_stack.pop()][\"resets\"] = eval(line[line.find(\"[\"):line.find(\"]\")+1])\n",
    "                else:\n",
    "                    job = line_split[0]\n",
    "                    summary_dict[\"trivial\"][job] = dict()\n",
    "                    job_stack.append(job)\n",
    "                    levels = list(map(int, re.findall(r'\\d+', line_split[1])))\n",
    "                    summary_dict[\"trivial\"][job][\"src_level\"] = levels[0] \n",
    "                    summary_dict[\"trivial\"][job][\"dest_level\"] = levels[1]\n",
    "\n",
    "                    # Tables are identical as the moves are trivial\n",
    "                    tables = line[line.find(\"[\"):line.find(\"]\")+1]\n",
    "                    summary_dict[\"trivial\"][job][\"tables\"] = eval(tables)\n",
    "            \n",
    "        elif flush:\n",
    "            if \"compaction:\" in line:\n",
    "                flush = False\n",
    "                compaction = True\n",
    "            else:\n",
    "                line_split = line.split()\n",
    "                if \"R\" in line:\n",
    "                    summary_dict[\"flush\"][job_stack.pop()][\"resets\"] = eval(line[line.find(\"[\"):line.find(\"]\")+1])\n",
    "                else:\n",
    "                    job = line_split[0]\n",
    "                    job_stack.append(job)\n",
    "                    summary_dict[\"flush\"][job] = dict()\n",
    "                    summary_dict[\"flush\"][job][\"table\"] = int(line[line.find(\"[\")+2:line.find(\"]\")-1]) # strip the ' char\n",
    "\n",
    "        elif compaction:\n",
    "            line_split = line.split()\n",
    "\n",
    "            if \"R\" in line:\n",
    "                summary_dict[\"compaction\"][job_stack.pop()][\"resets\"] = \\\n",
    "                    eval(line[line.find(\"[\"):line.find(\"]\")+1])\n",
    "            elif \"D\" in line:\n",
    "                job = job_stack.pop()\n",
    "                job_stack.append(job)\n",
    "                summary_dict[\"compaction\"][job][\"deletes\"] = \\\n",
    "                    eval(line[line.find(\"[\"):line.find(\"]\")+1])\n",
    "            else:\n",
    "                job = line_split[0]\n",
    "                summary_dict[\"compaction\"][job] = dict()\n",
    "                job_stack.append(job)\n",
    "                levels = list(map(int, re.findall(r'\\d+', line_split[1])))\n",
    "                summary_dict[\"compaction\"][job][\"src_level\"] = levels[0] \n",
    "                summary_dict[\"compaction\"][job][\"dest_level\"] = levels[1]\n",
    "                table_split = line.split(\"|\")\n",
    "                src_tables = line[table_split[0].find(\"[\"):table_split[0].find(\"]\")+1]\n",
    "                summary_dict[\"compaction\"][job][\"src_tables\"] = eval(src_tables)\n",
    "                info = table_split[1].split(\"->\")\n",
    "                summary_dict[\"compaction\"][job][\"merge_tables\"] = eval(info[0])\n",
    "#                 if info[0][2:-2] != \"\": # This was for manual if eval did not work\n",
    "#                     summary_dict[\"compaction\"][job][\"merge_tables\"] = list(map(int, info[0][2:-2].split(',')))\n",
    "#                 else:\n",
    "#                     summary_dict[\"compaction\"][job][\"merge_tables\"] = []\n",
    "                summary_dict[\"compaction\"][job][\"dest_tables\"] = eval(info[1])\n",
    "\n",
    "        if \"trivial:\" in line:\n",
    "            trivial = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fecc818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the json data\n",
    "file = open(DATA_FILE_JSON) \n",
    "json_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d305620b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8927e39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set configuration variables for plotting\n",
    "\n",
    "MAX_TIME_UNITS = 5 # Set the number of time units to show (i.e., operations; flush, compaction, trivial, F2FS GC)\n",
    "TRACE_FILE = 43 # Number of the SST file to trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e935b627",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Zone:\n",
    "    def __init__(self, zone, temperature, status, file):\n",
    "        self.zone = zone\n",
    "        self.temperature = temperature\n",
    "        self.status = status # Valid, Invalid, Reset\n",
    "        self.files = []\n",
    "        self.files.append(file)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"Zone Number: {self.zone}, Temp: {self.temperature}, Status: {self.status}, Files: {self.files}\\n\"\n",
    "    \n",
    "    def changestatus(self, status):\n",
    "        self.status = status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f2ef7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeUnit:\n",
    "    def __init__(self, id, job, operation, src, srclevel, destlevel):\n",
    "        self.id = id\n",
    "        self.operation = operation\n",
    "        self.job = job\n",
    "        self.zones = []\n",
    "        self.files = []\n",
    "        self.srcfile = src\n",
    "        self.srclevel = srclevel # M for memtable, X for non RocksDB op, otherwise the level number\n",
    "        self.destlevel = destlevel\n",
    "        \n",
    "    def __str__(self):\n",
    "        msg = f\"ID: {self.id}, Job: {self.job}, OP: {self.operation}, Level: {self.srclevel}->{self.destlevel} \\\n",
    "Source File: {self.srcfile}, Files: {self.files}\\n\"\n",
    "        msg += f\"Total Zones: {len(self.zones)}\\n\"\n",
    "        for zone in self.zones:\n",
    "            msg += str(zone)\n",
    "        return msg\n",
    "    \n",
    "    def addfile(self, file):\n",
    "        self.files.append(file)\n",
    "        \n",
    "    def hasfile(self, filename):\n",
    "        for file in self.files:\n",
    "            if filename == file:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def addzone(self, zoneid, temperature, status, file):\n",
    "        self.zones.append(Zone(zoneid, temperature, status, file))\n",
    "        if not file in self.files:\n",
    "            self.files.append(file)\n",
    "        \n",
    "    def haszone(self, zoneid):\n",
    "        for zone in self.zones:\n",
    "            if zoneid == zone.zone:\n",
    "                return True\n",
    "        return False\n",
    "        \n",
    "    def getzone(self, zoneid):\n",
    "        for zone in self.zones:\n",
    "            if zoneid == zone.zone:\n",
    "                return zone\n",
    "        return None\n",
    "    \n",
    "    def updatezonestatus(self, zoneid, status):\n",
    "        for i in range(len(self.zones)):\n",
    "            if self.zones[i].zone == zoneid:\n",
    "                self.zones[i].changestatus(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302861ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hastable(file, list):\n",
    "    for i in list:\n",
    "        if int(file) == int(i):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad50d165",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeline = []\n",
    "\n",
    "# Ignore trivial moves as they do not modify anything in the storage\n",
    "def constructdata(timeline):\n",
    "    maxzone = 0 # highest zone to plot to\n",
    "    timectr = 0\n",
    "    files = []\n",
    "\n",
    "    files.append(TRACE_FILE)\n",
    "    for op, item in summary_dict.items():\n",
    "        if op == \"flush\":\n",
    "            for job, data in item.items():\n",
    "                for file in files:\n",
    "                    if file == data[\"table\"]:\n",
    "                        timeline.append(TimeUnit(timectr, job, \"flush\", file, \"M\", \"0\"))\n",
    "\n",
    "                        # find the zone in the operation\n",
    "                        for entry in json_data[job][\"f2fs_file_snapshot_after\"]:\n",
    "                            if f\"0{file}.sst\" in entry[\"filename\"]:\n",
    "                                for zone in entry[\"hint\"]:\n",
    "                                    if not timeline[timectr].haszone(zone[0]):\n",
    "                                        timeline[timectr].addzone(zone[0], zone[1], \"Valid\", file)\n",
    "                                        files.append(file)\n",
    "                                        if zone[0] > maxzone:\n",
    "                                            maxzone = zone[0]\n",
    "\n",
    "                        timectr+=1\n",
    "                        if timectr > MAX_TIME_UNITS:\n",
    "                            return maxzone\n",
    "                        for reset in data[\"resets\"]:\n",
    "                            # flush counts as op already after snapshot, so -2\n",
    "                            if timeline[timectr - 2].haszone(reset):\n",
    "                                timeline.append(TimeUnit(timectr, job, \"reset\", \"X\", \"X\"))\n",
    "                                timeline[timectr].addzone(reset, \"None\", \"Reset\", \"\")\n",
    "                                timectr+=1\n",
    "                                if timectr > MAX_TIME_UNITS:\n",
    "                                    return maxzone\n",
    "                        break\n",
    "\n",
    "\n",
    "        if op == \"compaction\":\n",
    "            for job, data in item.items():\n",
    "                for file in files:\n",
    "                    if hastable(file, data[\"src_tables\"]) or hastable(file, data[\"merge_tables\"]):\n",
    "                        timeline.append(TimeUnit(timectr, job, \"compaction\", file, data[\"src_level\"], data[\"dest_level\"]))\n",
    "                        for destination in data[\"dest_tables\"]:\n",
    "\n",
    "                            # find the zone in the operation\n",
    "                            for entry in json_data[job][\"f2fs_file_snapshot_after\"]:\n",
    "                                if f\"0{destination}.sst\" in entry[\"filename\"]:\n",
    "                                    for zone in entry[\"hint\"]:\n",
    "                                        if not timeline[timectr].haszone(zone[0]):\n",
    "                                            timeline[timectr].addzone(zone[0], zone[1], \"Valid\", destination)\n",
    "                                            files.append(destination)\n",
    "                                            if zone[0] > maxzone:\n",
    "                                                maxzone = zone[0]\n",
    "\n",
    "        # TODO: Delete is messy, only doing if we need it in the figure \n",
    "        #                 for delete in data[\"deletes\"]:\n",
    "        #                     if timeline[timectr - 1].hasfile(delete):\n",
    "        #                         timeline.append(TimeUnit(timectr, job, \"delete\"))\n",
    "        #                         zonecopy = copy.deepcopy(timeline[timectr - 1].zones)\n",
    "        #                         for item in zonecopy:\n",
    "        #                             timeline[timectr].addzone(item.zone, item.temperature, item.status, item.files)\n",
    "        #                         timeline[timectr].updatezonestatus(delete, \"Invalid\")  \n",
    "\n",
    "                        timectr+=1\n",
    "                        if timectr > MAX_TIME_UNITS:\n",
    "                            return maxzone\n",
    "\n",
    "                        for reset in data[\"resets\"]:\n",
    "                            # compaction counts as op already after snapshot, so -2\n",
    "                            if timeline[timectr - 2].haszone(reset):\n",
    "                                timeline.append(TimeUnit(timectr, job, \"reset\", \"X\", \"X\"))\n",
    "                                timeline[timectr].updatezonestatus(reset, \"Reset\")\n",
    "                                timectr+=1\n",
    "                                if timectr > MAX_TIME_UNITS:\n",
    "                                    return maxzone\n",
    "\n",
    "                        break\n",
    "    return maxzone\n",
    "                            \n",
    "maxzone = constructdata(timeline)\n",
    "for i in timeline:\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f870a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 5), facecolor=\"0.7\") # For debugging made it gray and show ticks so we see things. drop later!\n",
    "ax = fig.add_axes([0, 0, 1, 1], frameon=False, aspect=1) #, xticks=[], yticks=[])\n",
    "\n",
    "color_mapping = {\n",
    "    'CURSEG_WARM_DATA': 'ORANGE',\n",
    "    'CURSEG_HOT_DATA': 'RED',\n",
    "    'CURSEG_COLD_DATA': 'CYAN'\n",
    "}\n",
    "\n",
    "gridsize = 2\n",
    "# print(maxzone)\n",
    "for timepoint, timeunit in zip(range(len(timeline)), timeline):\n",
    "    # Add all zones at time point\n",
    "#     print(timeunit)\n",
    "    zonewidth = gridsize/maxzone    \n",
    "    filewidth = (gridsize / len(timeunit.files))\n",
    "    files = {}\n",
    "    for x, file in zip(range(len(timeunit.files)), timeunit.files):\n",
    "#         print(file)\n",
    "        ax.add_patch(Rectangle((gridsize*timepoint+ 0.5*timepoint+x*filewidth+0.2, 2.1), filewidth*0.9, 0.5,\n",
    "                     edgecolor = 'black',\n",
    "                     facecolor = 'white',\n",
    "                     fill=True,\n",
    "                     lw=1))\n",
    "        if filewidth > 0.20:\n",
    "            ax.annotate(str(file),\n",
    "                xy=(gridsize*timepoint+ 0.5*timepoint+x*filewidth+0.2 + filewidth*0.9*0.5, 2.1+0.25), ha='center', va='center')\n",
    "        files[file] = (gridsize*timepoint+ 0.5*timepoint+x*filewidth+0.2, 2.1)    \n",
    "    for i in range(maxzone+1):\n",
    "        color = color_mapping[timeunit.getzone(i).temperature] if timeunit.haszone(i) else 'white'\n",
    "        ax.add_patch(Rectangle((i*zonewidth+0.1 + gridsize*timepoint + 0.5*timepoint, 0.1), zonewidth, 0.5,\n",
    "                     edgecolor = 'black',\n",
    "                     facecolor = color,\n",
    "                     fill=True,\n",
    "                     lw=1))\n",
    "        if timeunit.haszone(i):\n",
    "            zonefiles = timeunit.getzone(i).files\n",
    "            for file in zonefiles:\n",
    "                zx = i*zonewidth+0.1 + gridsize*timepoint + 0.5*timepoint + 0.5 * zonewidth\n",
    "                zy = 0.6\n",
    "                print('...', i, zonefiles, file)\n",
    "                fx = files[file][0]\n",
    "                if fx < zx and fx + filewidth*0.9 > zx:\n",
    "                    fx = zx\n",
    "                else:\n",
    "                    fx = fx + filewidth*0.9*0.5\n",
    "                fy = files[file][1]\n",
    "                ax.plot([zx,fx], [zy, fy], color=color)\n",
    "    if timepoint != 0:\n",
    "#             print('timepoint', (timepoint-1)*gridsize)\n",
    "#             ax.plot([0.2+gridsize*0.5 + (timepoint-1)*gridsize*1.25+0.1, \n",
    "#                      0.2+gridsize*0.5 + (timepoint)*gridsize*1.25],\n",
    "#                     [2.6, 4],\n",
    "#                     color='black')\n",
    "\n",
    "            ax.arrow(x=0.2+gridsize*0.5 + (timepoint-1)*gridsize*1.25+0.1, \n",
    "                     dx=-0.15 + 0.2+gridsize*0.5 + (timepoint)*gridsize*1.25 - (0.2+gridsize*0.5 + (timepoint-1)*gridsize*1.25+0.1),\n",
    "                    y=2.6, dy=1.4-0.13,\n",
    "                    color='black', head_width=0.15, head_length=0.13)\n",
    "    ax.arrow(x=0.2+gridsize*0.5+ timepoint*gridsize*1.25, \n",
    "             dx=0, y=4, dy=-1.4+0.13, color='black', head_width=0.15, head_length=0.13)\n",
    "    ax.annotate(str(timeunit.operation),\n",
    "                xy=(0.2+gridsize*0.5+ timepoint*gridsize*1.25,4.2), ha='center', va='center')\n",
    "        \n",
    "ax.set_ylim(bottom=0, top=10)\n",
    "ax.set_xlim(0,10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c29c8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15265d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
