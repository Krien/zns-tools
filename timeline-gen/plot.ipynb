{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf6fc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.patches as mpatches\n",
    "import copy\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c06b22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['ps.useafm'] = True\n",
    "matplotlib.rcParams['pdf.use14corefonts'] = True\n",
    "matplotlib.rcParams['text.usetex'] = True\n",
    "\n",
    "text_font_size = 16\n",
    "marker_font_size = 11\n",
    "label_font_size = 15\n",
    "axes_font_size = 12\n",
    "\n",
    "plt.rc('font', size=text_font_size)         \n",
    "plt.rc('axes', labelsize=axes_font_size)    \n",
    "plt.rc('xtick', labelsize=label_font_size)    \n",
    "plt.rc('ytick', labelsize=label_font_size)    \n",
    "plt.rc('legend', fontsize=label_font_size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c598a571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set global vars\n",
    "DATA_FILE_JSON = \"data.json\"\n",
    "DATA_FILE_SUMMARY = \"summary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01c8ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the summary to identify files to follow\n",
    "#\n",
    "# Assumes summary format is in this order\n",
    "#     trivial:    -> all trivial moves\n",
    "#                 order of:\n",
    "#                     job [src_level]->[dest_level] [src_table]->[dest_table]\n",
    "#                     R: [zone_resets]\n",
    "#     flush:      -> all flushes\n",
    "#                 order of:\n",
    "#                     job [table]\n",
    "#                     R: [zone_resets]\n",
    "#     compaction: -> all compactions\n",
    "#                 order of:\n",
    "#                     job src_level->dest_level [src_tables] | [merge_tables] -> [dest_tables] \n",
    "#                     D: [table_deletes]\n",
    "#                     R: [zone_resets]\n",
    "\n",
    "summary_dict = dict()\n",
    "\n",
    "with open(DATA_FILE_SUMMARY) as sum_file:\n",
    "    trivial = False\n",
    "    flush = False\n",
    "    compaction = False\n",
    "    summary_dict[\"trivial\"] = dict()\n",
    "    summary_dict[\"flush\"] = dict()\n",
    "    summary_dict[\"compaction\"] = dict()\n",
    "    job_stack = []\n",
    "\n",
    "    \n",
    "    for line in sum_file:\n",
    "        if trivial:\n",
    "            if \"flush:\" in line:\n",
    "                trivial = False\n",
    "                flush = True\n",
    "            else:\n",
    "                line_split = line.split()\n",
    "                if \"R\" in line:\n",
    "                    summary_dict[\"trivial\"][job_stack.pop()][\"resets\"] = eval(line[line.find(\"[\"):line.find(\"]\")+1])\n",
    "                else:\n",
    "                    job = line_split[0]\n",
    "                    summary_dict[\"trivial\"][job] = dict()\n",
    "                    job_stack.append(job)\n",
    "                    levels = list(map(int, re.findall(r'\\d+', line_split[1])))\n",
    "                    summary_dict[\"trivial\"][job][\"src_level\"] = levels[0] \n",
    "                    summary_dict[\"trivial\"][job][\"dest_level\"] = levels[1]\n",
    "\n",
    "                    # Tables are identical as the moves are trivial\n",
    "                    tables = line[line.find(\"[\"):line.find(\"]\")+1]\n",
    "                    summary_dict[\"trivial\"][job][\"tables\"] = eval(tables)\n",
    "            \n",
    "        elif flush:\n",
    "            if \"compaction:\" in line:\n",
    "                flush = False\n",
    "                compaction = True\n",
    "            else:\n",
    "                line_split = line.split()\n",
    "                if \"R\" in line:\n",
    "                    summary_dict[\"flush\"][job_stack.pop()][\"resets\"] = eval(line[line.find(\"[\"):line.find(\"]\")+1])\n",
    "                else:\n",
    "                    job = line_split[0]\n",
    "                    job_stack.append(job)\n",
    "                    summary_dict[\"flush\"][job] = dict()\n",
    "                    summary_dict[\"flush\"][job][\"table\"] = int(line[line.find(\"[\")+2:line.find(\"]\")-1]) # strip the ' char\n",
    "\n",
    "        elif compaction:\n",
    "            line_split = line.split()\n",
    "\n",
    "            if \"R\" in line:\n",
    "                summary_dict[\"compaction\"][job_stack.pop()][\"resets\"] = \\\n",
    "                    eval(line[line.find(\"[\"):line.find(\"]\")+1])\n",
    "            elif \"D\" in line:\n",
    "                job = job_stack.pop()\n",
    "                job_stack.append(job)\n",
    "                summary_dict[\"compaction\"][job][\"deletes\"] = \\\n",
    "                    eval(line[line.find(\"[\"):line.find(\"]\")+1])\n",
    "            else:\n",
    "                job = line_split[0]\n",
    "                summary_dict[\"compaction\"][job] = dict()\n",
    "                job_stack.append(job)\n",
    "                levels = list(map(int, re.findall(r'\\d+', line_split[1])))\n",
    "                summary_dict[\"compaction\"][job][\"src_level\"] = levels[0] \n",
    "                summary_dict[\"compaction\"][job][\"dest_level\"] = levels[1]\n",
    "                table_split = line.split(\"|\")\n",
    "                src_tables = line[table_split[0].find(\"[\"):table_split[0].find(\"]\")+1]\n",
    "                summary_dict[\"compaction\"][job][\"src_tables\"] = eval(src_tables)\n",
    "                info = table_split[1].split(\"->\")\n",
    "                summary_dict[\"compaction\"][job][\"merge_tables\"] = eval(info[0])\n",
    "#                 if info[0][2:-2] != \"\": # This was for manual if eval did not work\n",
    "#                     summary_dict[\"compaction\"][job][\"merge_tables\"] = list(map(int, info[0][2:-2].split(',')))\n",
    "#                 else:\n",
    "#                     summary_dict[\"compaction\"][job][\"merge_tables\"] = []\n",
    "                summary_dict[\"compaction\"][job][\"dest_tables\"] = eval(info[1])\n",
    "\n",
    "        if \"trivial:\" in line:\n",
    "            trivial = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fecc818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the json data\n",
    "file = open(DATA_FILE_JSON) \n",
    "json_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d305620b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa3b25f0",
   "metadata": {},
   "source": [
    "## Class Defintitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e935b627",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Zone:\n",
    "    def __init__(self, zone, temperature, status, file):\n",
    "        self.zone = zone\n",
    "        self.temperature = temperature\n",
    "        self.status = status # Valid, Invalid, Reset\n",
    "        self.files = []\n",
    "        self.files.append(file)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"Zone Number: {self.zone}, Temp: {self.temperature}, Status: {self.status}, Files: {self.files}\\n\"\n",
    "    \n",
    "    def changestatus(self, status):\n",
    "        self.status = status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f2ef7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeUnit:\n",
    "    def __init__(self, id, job, operation, src, srclevel, destlevel, mergetables, time, timestamp_object):\n",
    "        self.id = id\n",
    "        self.operation = operation\n",
    "        self.job = job\n",
    "        self.zones = []\n",
    "        self.files = []\n",
    "        self.srctables = src\n",
    "        self.mergetables = mergetables\n",
    "        self.srclevel = srclevel # M for memtable, X for non RocksDB op, otherwise the level number\n",
    "        self.destlevel = destlevel\n",
    "        self.timestamp = time\n",
    "        self.timestamp_object = timestamp_object\n",
    "        \n",
    "    def __str__(self):\n",
    "        msg = f\"ID: {self.id}, Job: {self.job}, OP: {self.operation}, Time: {self.timestamp}, \\\n",
    "Level: {self.srclevel}->{self.destlevel} Files: {self.files}, Source Files: {self.srctables}, \\\n",
    "Merged Files: {self.mergetables}\\n\"\n",
    "        msg += f\"Total Zones: {len(self.zones)}\\n\"\n",
    "        for zone in self.zones:\n",
    "            msg += str(zone)\n",
    "        return msg\n",
    "    \n",
    "    def addfile(self, file):\n",
    "        if file != \"\":\n",
    "            self.files.append(file)\n",
    "        \n",
    "    def hasfile(self, filename):\n",
    "        for file in self.files:\n",
    "            if filename == file:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "    def addzone(self, zoneid, temperature, status, file):\n",
    "        self.zones.append(Zone(zoneid, temperature, status, file))\n",
    "        if not file in self.files and file != \"\":\n",
    "            self.files.append(file)\n",
    "        \n",
    "    def haszone(self, zoneid):\n",
    "        for zone in self.zones:\n",
    "            if zoneid == zone.zone:\n",
    "                return True\n",
    "        return False\n",
    "        \n",
    "    def getzone(self, zoneid):\n",
    "        for zone in self.zones:\n",
    "            if zoneid == zone.zone:\n",
    "                return zone\n",
    "        return None\n",
    "    \n",
    "    def updatezonestatus(self, zoneid, status):\n",
    "        for i in range(len(self.zones)):\n",
    "            if self.zones[i].zone == zoneid:\n",
    "                self.zones[i].changestatus(status)\n",
    "                \n",
    "    def setoldzones(self, oldzones, isreset):\n",
    "        for zone in oldzones:\n",
    "            if zone.temperature == 'RESET_ZONE':\n",
    "                pass\n",
    "            elif isreset:\n",
    "                self.addzone(zone.zone, zone.temperature, zone.status, \"\")\n",
    "            else:\n",
    "                self.addzone(zone.zone, \"INVALID_ZONE\", \"Invalid\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdff75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hastable(file, list):\n",
    "    for i in list:\n",
    "        if int(file) == int(i):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cf0a17",
   "metadata": {},
   "source": [
    "## Set configuration variables for plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008b9249",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TIME_UNITS = 4 # Set the number of time units to show (i.e., operations; flush, compaction, trivial, F2FS GC)\n",
    "TRACE_FILE = 31 # Number of the SST file to trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad50d165",
   "metadata": {},
   "outputs": [],
   "source": [
    "timeline = []\n",
    "\n",
    "# Ignore trivial moves as they do not modify anything in the storage\n",
    "def constructdata(timeline):\n",
    "    maxzone = 0 # highest zone to plot to\n",
    "    timectr = 0\n",
    "    files = []\n",
    "\n",
    "    files.append(TRACE_FILE)\n",
    "    for op, item in summary_dict.items():\n",
    "        if op == \"flush\":\n",
    "            for job, data in item.items():\n",
    "                for file in files:\n",
    "                    if file == data[\"table\"]: \n",
    "                        timestamp = datetime.strptime(json_data[job][\"datetime\"], '%Y-%m-%d %H:%M:%S.%f')\n",
    "                        if timectr == 0:\n",
    "                            time = 0\n",
    "                        else:\n",
    "                            delta = timestamp - timeline[timectr - 1].timestamp_object\n",
    "                            time = delta.total_seconds()\n",
    "                        timeline.append(TimeUnit(timectr, job, \"Flush\", [file], \"M\", \"0\", [], \n",
    "                                                 time, timestamp))\n",
    "                        if timectr > 0:\n",
    "                            timeline[timectr].setoldzones(timeline[timectr - 1].zones, False)\n",
    "\n",
    "                        # find the zone in the operation\n",
    "                        for entry in json_data[job][\"f2fs_file_snapshot_after\"]:\n",
    "                            if f\"0{file}.sst\" in entry[\"filename\"]:\n",
    "                                for zone in entry[\"hint\"]:\n",
    "                                    if not timeline[timectr].haszone(zone[0]):\n",
    "                                        timeline[timectr].addzone(zone[0], zone[1], \"Valid\", file)\n",
    "                                        files.append(file)\n",
    "                                        if zone[0] > maxzone:\n",
    "                                            maxzone = zone[0]\n",
    "\n",
    "                        timectr+=1\n",
    "                        if timectr >= MAX_TIME_UNITS:\n",
    "                            return maxzone\n",
    "                        \n",
    "                        if len(data[\"resets\"]) > 0:\n",
    "                            timestamp = datetime.strptime(json_data[job][\"datetime\"], '%Y-%m-%d %H:%M:%S.%f')\n",
    "                            if timectr == 0:\n",
    "                                time = 0\n",
    "                            else:\n",
    "                                delta = timestamp - timeline[timectr - 1].timestamp_object\n",
    "                                time = delta.total_seconds()\n",
    "                            timeline.append(TimeUnit(timectr, job, \"reset\", \"X\", \"X\", \"X\", [], \n",
    "                                                     time, timestamp))\n",
    "                            if timectr > 0:\n",
    "                                timeline[timectr].setoldzones(timeline[timectr - 1].zones , True)\n",
    "                            for reset in data[\"resets\"]:\n",
    "                                timeline[timectr].addzone(reset, \"RESET_ZONE\", \"Reset\", \"\")\n",
    "                            timectr+=1\n",
    "                            if timectr >= MAX_TIME_UNITS:\n",
    "                                return maxzone\n",
    "                        break\n",
    "\n",
    "\n",
    "        if op == \"compaction\":\n",
    "            for job, data in item.items():\n",
    "                for file in files:\n",
    "                    if hastable(file, data[\"src_tables\"]) or hastable(file, data[\"merge_tables\"]):\n",
    "                        timestamp = datetime.strptime(json_data[job][\"datetime\"], '%Y-%m-%d %H:%M:%S.%f')\n",
    "                        if timectr == 0:\n",
    "                            time = 0\n",
    "                        else:\n",
    "                            delta = timestamp - timeline[timectr - 1].timestamp_object\n",
    "                            time = delta.total_seconds()\n",
    "                        timeline.append(TimeUnit(timectr, job, \"Compaction\", data[\"src_tables\"], \n",
    "                                                 data[\"src_level\"], data[\"dest_level\"], data[\"merge_tables\"], \n",
    "                                                 time, timestamp))\n",
    "                        if timectr > 0:\n",
    "                            timeline[timectr].setoldzones(timeline[timectr - 1].zones, False)\n",
    "                        for destination in data[\"dest_tables\"]:\n",
    "\n",
    "                            # find the zone in the operation\n",
    "                            for entry in json_data[job][\"f2fs_file_snapshot_after\"]:\n",
    "                                if f\"0{destination}.sst\" in entry[\"filename\"]:\n",
    "                                    for zone in entry[\"hint\"]:\n",
    "                                        if not timeline[timectr].haszone(zone[0]):\n",
    "                                            timeline[timectr].addzone(zone[0], zone[1], \"Valid\", destination)\n",
    "                                            files.append(destination)\n",
    "                                            if zone[0] > maxzone:\n",
    "                                                maxzone = zone[0]\n",
    "\n",
    "        # TODO: Delete is messy, only doing if we need it in the figure \n",
    "        #                 for delete in data[\"deletes\"]:\n",
    "        #                     if timeline[timectr - 1].hasfile(delete):\n",
    "        #                         timeline.append(TimeUnit(timectr, job, \"delete\"))\n",
    "        #                         zonecopy = copy.deepcopy(timeline[timectr - 1].zones)\n",
    "        #                         for item in zonecopy:\n",
    "        #                             timeline[timectr].addzone(item.zone, item.temperature, item.status, item.files)\n",
    "        #                         timeline[timectr].updatezonestatus(delete, \"Invalid\")  \n",
    "\n",
    "                        timectr+=1\n",
    "                        if timectr >= MAX_TIME_UNITS:\n",
    "                            return maxzone\n",
    "\n",
    "                        if len(data[\"resets\"]) > 0:\n",
    "                            timestamp = datetime.strptime(json_data[job][\"datetime\"], '%Y-%m-%d %H:%M:%S.%f')\n",
    "                            if timectr == 0:\n",
    "                                time = 0\n",
    "                            else:\n",
    "                                delta = timestamp - timeline[timectr - 1].timestamp_object\n",
    "                                time = delta.total_seconds()\n",
    "                            timeline.append(TimeUnit(timectr, job, \"reset\", \"X\", \"X\", \"X\", [], \n",
    "                                                     time, timestamp))\n",
    "                            if timectr > 0:\n",
    "                                timeline[timectr].setoldzones(timeline[timectr - 1].zones, True)\n",
    "                            for reset in data[\"resets\"]:\n",
    "                                timeline[timectr].addzone(reset, \"RESET_ZONE\", \"Reset\", \"\")\n",
    "                            timectr+=1\n",
    "                            if timectr >= MAX_TIME_UNITS:\n",
    "                                return maxzone\n",
    "\n",
    "                        break\n",
    "    return maxzone\n",
    "                            \n",
    "maxzone = constructdata(timeline)\n",
    "for i in timeline:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f870a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(min(MAX_TIME_UNITS, len(timeline))*2.5, 5), facecolor=\"1\") # For debugging made it gray and show ticks so we see things. drop later!\n",
    "ax = fig.add_axes([0, 0, 1, 1], frameon=False, aspect=1, xticks=[], yticks=[])\n",
    "\n",
    "color_mapping = {\n",
    "    'CURSEG_WARM_DATA': 'ORANGE',\n",
    "    'CURSEG_HOT_DATA': 'RED',\n",
    "    'CURSEG_COLD_DATA': 'CYAN',\n",
    "    'RESET_ZONE': 'GREEN',\n",
    "    'INVALID_ZONE': 'MAGENTA'\n",
    "}\n",
    "\n",
    "gridsize = 2\n",
    "# print(maxzone)\n",
    "for timepoint, timeunit in zip(range(len(timeline)), timeline):\n",
    "    # Add all zones at time point\n",
    "#     print(timeunit)\n",
    "    zonewidth = gridsize/maxzone\n",
    "    if len(timeunit.files) > 0:\n",
    "        filewidth = (gridsize / len(timeunit.files))\n",
    "    files = {}\n",
    "    if timeunit.operation == \"reset\":\n",
    "        ax.add_patch(Rectangle((gridsize*timepoint+ 0.5*timepoint+x*filewidth+0.2, 2.1), filewidth*0.9, 0.5,\n",
    "                     edgecolor = 'black',\n",
    "                     facecolor = 'white',\n",
    "                     fill=True, zorder=1,\n",
    "                     lw=1))\n",
    "        if filewidth > 0.20:\n",
    "            ax.annotate(\"Zone Reset\",\n",
    "                xy=(gridsize*timepoint+ 0.5*timepoint+x*filewidth+0.2 + filewidth*0.9*0.5, 2.1+0.25), ha='center', va='center')\n",
    "        files[file] = (gridsize*timepoint+ 0.5*timepoint+x*filewidth+0.2, 2.1)    \n",
    "        for i in range(maxzone+1):\n",
    "            color = color_mapping[timeunit.getzone(i).temperature] if timeunit.haszone(i) else 'white'\n",
    "\n",
    "            ax.add_patch(Rectangle((i*zonewidth+0.1 + gridsize*timepoint + 0.5*timepoint, 0.1), zonewidth, 0.5,\n",
    "                         edgecolor = 'black',\n",
    "                         facecolor = color,\n",
    "                         fill=True, zorder=1,\n",
    "                         lw=1))\n",
    "            if timeunit.haszone(i):\n",
    "                zonefiles = timeunit.getzone(i).files\n",
    "                for file in zonefiles:\n",
    "                    zx = i*zonewidth+0.1 + gridsize*timepoint + 0.5*timepoint + 0.5 * zonewidth\n",
    "                    zy = 0.6\n",
    "                    print('...', i, zonefiles, file)\n",
    "                    filewidth = gridsize\n",
    "                    fx =  gridsize*timepoint+ 0.5*timepoint+x*filewidth+0.2\n",
    "                    if fx < zx and fx + filewidth*0.9 > zx:\n",
    "                        fx = zx\n",
    "                    else:\n",
    "                        fx = fx + filewidth*0.9*0.5\n",
    "                    fy = gridsize*timepoint+ 0.5*timepoint+x*filewidth+0.2\n",
    "                    if color == 'GREEN':\n",
    "                        ax.plot([zx,fx], [zy, fy-0.6], color=color)\n",
    "    else:\n",
    "        # All timestamps that are not zone resets\n",
    "        skip = 0\n",
    "        for x, file in zip(range(len(timeunit.files)), timeunit.files):\n",
    "            ax.add_patch(Rectangle((gridsize*timepoint+ 0.5*timepoint+(x-skip)*filewidth+0.2, 2.1), filewidth*0.9, 0.5,\n",
    "                         edgecolor = 'black',\n",
    "                         facecolor = 'white',\n",
    "                         fill=True, zorder=1,\n",
    "                         lw=1))\n",
    "            if filewidth > 0.20:\n",
    "                ax.annotate(str(file),\n",
    "                    xy=(gridsize*timepoint+ 0.5*timepoint+(x-skip)*filewidth+0.2 + filewidth*0.9*0.5, 2.1+0.25), ha='center', va='center')\n",
    "            files[file] = (gridsize*timepoint+ 0.5*timepoint+(x-skip)*filewidth+0.2, 2.1)    \n",
    "        for i in range(maxzone+1):\n",
    "            color = color_mapping[timeunit.getzone(i).temperature] if timeunit.haszone(i) else 'white'\n",
    "\n",
    "            ax.add_patch(Rectangle((i*zonewidth+0.1 + gridsize*timepoint + 0.5*timepoint, 0.1), zonewidth, 0.5,\n",
    "                         edgecolor = 'black',\n",
    "                         facecolor = color,\n",
    "                         fill=True, zorder=1,\n",
    "                         lw=1))\n",
    "            if timeunit.haszone(i):\n",
    "                zonefiles = timeunit.getzone(i).files\n",
    "                for file in zonefiles:\n",
    "                    if file != \"\":\n",
    "                        zx = i*zonewidth+0.1 + gridsize*timepoint + 0.5*timepoint + 0.5 * zonewidth\n",
    "                        zy = 0.6\n",
    "                        print('...', i, zonefiles, file)\n",
    "                        fx = files[file][0]\n",
    "                        if fx < zx and fx + filewidth*0.9 > zx:\n",
    "                            fx = zx\n",
    "                        else:\n",
    "                            fx = fx + filewidth*0.9*0.5\n",
    "                        fy = files[file][1]\n",
    "                        if color != \"MAGENTA\" and color != \"GREEN\":\n",
    "                            ax.plot([zx,fx], [zy, fy], color=color)\n",
    "        if timepoint != 0:\n",
    "    #             print('timepoint', (timepoint-1)*gridsize)\n",
    "    #             ax.plot([0.2+gridsize*0.5 + (timepoint-1)*gridsize*1.25+0.1, \n",
    "    #                      0.2+gridsize*0.5 + (timepoint)*gridsize*1.25],\n",
    "    #                     [2.6, 4],\n",
    "    #                     color='black')\n",
    "            if timeline[timepoint - 1].operation == \"reset\":\n",
    "                ax.arrow(x=0.2+gridsize*0.5 + (timepoint-1)*gridsize*1.25+0.1 - 2.5, \n",
    "                        dx=-0.15 + 0.2+gridsize*0.5 + (timepoint)*gridsize*1.25 - (0.2+gridsize*0.5 + (timepoint-1)*gridsize*1.25+0.1) + 2.5,\n",
    "                        y=2.6, dy=1.4-0.13+0.1,\n",
    "                        color='black', head_width=0.15, head_length=0.13)\n",
    "            else:\n",
    "                ax.arrow(x=0.2+gridsize*0.5 + (timepoint-1)*gridsize*1.25+0.1, \n",
    "                         dx=-0.15 + 0.2+gridsize*0.5 + (timepoint)*gridsize*1.25 - (0.2+gridsize*0.5 + (timepoint-1)*gridsize*1.25+0.1),\n",
    "                        y=2.6, dy=1.4-0.13+0.1,\n",
    "                        color='black', head_width=0.15, head_length=0.13)\n",
    "        if str(timeunit.srclevel) == \"M\":\n",
    "            ax.arrow(x=0.2+gridsize*0.5+ timepoint*gridsize*1.25, \n",
    "            dx=0, y=4, dy=-1.4+0.13, color='black', head_width=0.15, head_length=0.13)\n",
    "            ax.annotate(f\"{str(timeunit.operation)}\\n\\nmem-$>$L{str(timeunit.destlevel)}\",\n",
    "            xy=(0.2+gridsize*0.5+ timepoint*gridsize*1.25,4.5), ha='center', va='center')\n",
    "        else:\n",
    "            ax.arrow(x=0.2+gridsize*0.5+ timepoint*gridsize*1.25, \n",
    "            dx=0, y=4, dy=-1.4+0.13, color='black', head_width=0.15, head_length=0.13)\n",
    "            ax.annotate(f\"{str(timeunit.operation)}\\n{str(timeunit.srctables)} + {str(timeunit.mergetables)}\\n \\\n",
    "    L{str(timeunit.srclevel)}-$>$L{str(timeunit.destlevel)}\",\n",
    "            xy=(0.2+gridsize*0.5+ timepoint*gridsize*1.25,4.5), ha='center', va='center')\n",
    "        ax.annotate(f\"{str(round(timeunit.timestamp, 2))}s\", xy=(0.2+gridsize*0.5+ timepoint*gridsize*1.25,5.1), ha='center', va='center')\n",
    "        \n",
    "\n",
    "\n",
    "# ZNS zones annotation\n",
    "plt.plot([-1,min(MAX_TIME_UNITS, len(timeline))*2.5], [1.4,1.4], linestyle='dashed', color=\"black\")\n",
    "ax.add_patch(Rectangle((-1, 0.01), min(MAX_TIME_UNITS, len(timeline))*2.5+1, 0.8,\n",
    "                     edgecolor = 'black',\n",
    "                     facecolor = 'gray',\n",
    "                     fill=True, zorder=0,\n",
    "                     lw=1, alpha=0.15))\n",
    "ax.annotate(\"  ZNS\\nZones\", xy=(-1, 0), xytext=(-0.9, 0.2))\n",
    "\n",
    "# F2FS annotation\n",
    "plt.plot([-1,min(MAX_TIME_UNITS, len(timeline))*2.5], [3.3,3.3], linestyle='dashed', color=\"black\")\n",
    "ax.add_patch(Rectangle((-1, 1.95), min(MAX_TIME_UNITS, len(timeline))*2.5+1, 0.8,\n",
    "                     edgecolor = 'black',\n",
    "                     facecolor = 'gray',\n",
    "                     fill=True, zorder=0,\n",
    "                     lw=1, alpha=0.15))\n",
    "ax.annotate('F2FS', xy=(-1, 0), xytext=(-0.9, 2.3))\n",
    "\n",
    "# RocksDB annotation\n",
    "ax.add_patch(Rectangle((-1, 4.1), min(MAX_TIME_UNITS, len(timeline))*2.5+1, 0.8,\n",
    "                     edgecolor = 'black',\n",
    "                     facecolor = 'gray',\n",
    "                     fill=True, zorder=0,\n",
    "                     lw=1, alpha=0.15))\n",
    "ax.annotate('RocksDB', xy=(-1, 0), xytext=(-0.9, 4.45))\n",
    "\n",
    "ax.set_ylim(bottom=0, top=5.1)\n",
    "ax.set_xlim(-1)\n",
    "\n",
    "handles = []\n",
    "handles.append(mpatches.Patch(color=\"RED\", label=\"HOT Zone\"))\n",
    "handles.append(mpatches.Patch(color=\"ORANGE\", label=\"WARM Zone\"))\n",
    "handles.append(mpatches.Patch(color=\"CYAN\", label=\"COLD Zone\"))\n",
    "handles.append(mpatches.Patch(color=\"GREEN\", label=\"Reset Zone\"))\n",
    "handles.append(mpatches.Patch(color=\"MAGENTA\", label=\"Deleted file\"))\n",
    "\n",
    "ax.legend(loc=(0,1.1), handles=handles, ncol=5)\n",
    "\n",
    "# plt.show()\n",
    "plt.savefig(f\"RocksDB-{TRACE_FILE}_sst-timeline.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0a9cbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d28d642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2a0034",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
